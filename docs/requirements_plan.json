{
  "requirements": {
    "overview": "Build a local-first, resilient ETL CLI in Python that ingests genomic TSV studies from a folder tree, streams and filters by Ensembl IDs, standardizes heterogeneous metadata, and loads a normalized star schema in SQL Server. The tool must be idempotent, batch/transactional with resume after failures, persist run artifacts in a local SQLite database, and provide monitoring logs and optional summary reports. Implementation will follow current Python data stack patterns using Polars for fast TSV streaming, SQLAlchemy Core 2.x with pyodbc for SQL Server, and structured JSON logging.",
    "goals": [
      "Automate multi-study processing with configurable concurrency and no manual intervention",
      "Standardize messy metadata into dimensional tables suitable for analytics",
      "Filter expression data during ingestion to only target Ensembl genes for performance",
      "Ensure reliability via idempotency, batching, transactions, and resume-after-failure",
      "Populate a star-schema optimized for genomic expression analysis",
      "Provide environment-specific configuration via external files",
      "Deliver comprehensive logging and basic quality/performance summaries",
      "Honor local-first execution with SQLite artifact persistence and operational snapshot/rollback"
    ],
    "assumptions": [
      "All inputs are TSV files; each study directory contains one metadata TSV and one expression TSV",
      "Expression data uses Ensembl gene IDs (format ENSG00000XXXXXXX) in the first column named 'Gene'",
      "Studies are independent and can be processed concurrently",
      "A SQL Server instance is available for the analytical schema; batch inserts and transactions are supported",
      "Local filesystem access to the root input directory is available",
      "Temporary network failures may occur; the system can reconnect and resume",
      "Data volumes per study: 10k–50k samples, ~20k+ genes; filtering reduces to ~120 target genes (~99% reduction)",
      "Python 3.13.x is the default runtime (supports free-threaded builds; stable bugfix 3.13.8 released Oct 7, 2025). ([peps.python.org](https://peps.python.org/pep-0719/?utm_source=openai))",
      "Core libraries follow current versions/patterns: SQLAlchemy 2.0.44, Polars 1.34.0, pyodbc 5.2.0, Pydantic 2.12, structlog 25.4.0, tenacity 9.1.2. ([sqlalchemy.org](https://www.sqlalchemy.org/blog/?utm_source=openai))"
    ],
    "scope": {
      "must": [
        "Multi-study discovery: scan a configured root directory and detect study subfolders that each contain metadata_<GSE>.tsv and expression_<GSE>.tsv (case-sensitive) (REQ-001).",
        "Concurrent processing: process multiple studies in parallel with a configurable max_concurrent_studies; isolation ensures no cross-study interference (REQ-001).",
        "Gene filtering during load: load a TSV filter list of Ensembl IDs; stream expression TSV rows and include only genes in the filter before loading into memory structures (skip non-matching genes without per-record logging) (REQ-002).",
        "Database population: load dim_gene, dim_sample, dim_study, dim_illness, dim_platform and fact_expression with correct types and foreign keys per target schema (REQ-003).",
        "Transformation rules: apply specified source-to-target metadata mappings; substitute empty/missing values with the literal 'UNKNOWN' (REQ-004).",
        "Idempotency and duplicate skipping: enforce uniqueness rules (dim_study.gse_accession; dim_gene.ensembl_id; dim_sample(gsm_accession, study_key); dim_platform.platform_accession; dim_illness.illness_label; fact_expression(sample_key, gene_key, study_key)) via upsert/merge or unique indexes and skip on conflict (REQ-005).",
        "Batching and transactions: default batch_size=1000 (configurable); commit each batch in its own transaction; separate batching per table type (REQ-006).",
        "Resume capability: persist per-study processing state (last committed batch per table, file offsets/row indices) and resume after database/network failures from the last successful commit; implement reconnect with exponential backoff (REQ-007).",
        "Configuration management: all deployment parameters (DB connection string, timeouts, input paths, gene filter path, concurrency, batch sizes, logging options) are in external YAML and take effect without code changes (REQ-008).",
        "Monitoring & logging: structured JSON logs capture per-study start/end, record counts (loaded/filtered/errors) per table, performance (throughput, memory), and data quality findings; exclude per-gene skip logs (REQ-009).",
        "Local-first execution: support a 'local' mode that uses only local resources—processing and artifact persistence run with a local SQLite database; production mode writes to SQL Server while still persisting artifacts locally.",
        "Operational snapshots & rollback: each ETL run has a generated run_id; all inserts tag rows with run_id; provide a rollback command to delete rows for a given run_id across fact and dims (where safe) to recover from bad loads within a short retention window (operational rollback only; not long-term versioning).",
        "SQLite artifact persistence: maintain a local artifacts.db (SQLite) storing runs, batches, checkpoints, file hashes, metrics, and data-quality summaries; export summary CSV/JSON files to the /logs directory.",
        "Implementation stack (current patterns): Python 3.13.x; SQLAlchemy Core 2.0.44 with the mssql+pyodbc dialect and fast_executemany for bulk-like executemany; Polars 1.34.0 for TSV streaming; Pydantic v2.12 for config models; structlog 25.4.0 for JSON logging; tenacity 9.1.2 for retries. ([pypi.org](https://pypi.org/project/SQLAlchemy/?utm_source=openai))",
        "SQL Server connection pattern: ODBC Driver 17+ with fast_executemany=True on the pyodbc cursor/SQLAlchemy engine for improved batch insert throughput (documented practice). ([github.com](https://github.com/sqlalchemy/sqlalchemy/discussions/9436?utm_source=openai))",
        "Schema DDL and constraints: provide migration scripts for SQL Server and SQLite with aligned types; use IDENTITY/AUTOINCREMENT as appropriate; apply unique constraints/indexes to enforce idempotency."
      ],
      "should": [
        "Validation reporting: produce a per-study summary report with completeness metrics, filtered gene counts vs total, warnings, and performance stats (REQ-010).",
        "File format validation: preflight checks for required columns, delimiter/encoding (UTF-8), first-column 'Gene' in expression TSV, expected file sizes; fail fast with clear errors (REQ-011).",
        "Performance optimizations: thread-pool parallelism across studies, memory-efficient streaming, connection pooling, and bulk insert tuning to achieve ~50% faster multi-study throughput (REQ-012).",
        "Dynamic metadata field mappings: allow configurable lists of candidate column names (age/sex/illness/platform) to accommodate study variability (REQ-013).",
        "Configurable retry policy: exponential backoff with jitter and capped retries for DB/network operations using tenacity.",
        "Observability extras: optional Prometheus-compatible metrics endpoint (local-only) and periodic metrics snapshots persisted in artifacts.db."
      ],
      "wont": [
        "Real-time/streaming ingestion; the system operates in scheduled batch mode (REQ-014).",
        "Advanced semantic data validation (range/outlier checks) beyond required presence/format; data loaded as-is (REQ-015).",
        "Web UI; only CLI and logs are provided (REQ-016).",
        "Full historical data versioning of database contents; source TSVs remain the system of record (REQ-017).",
        "Custom export formats; downstream consumers query the database directly (REQ-018)."
      ]
    },
    "user_stories": [
      {
        "role": "genomics data analyst",
        "goal": "process multiple expression studies simultaneously",
        "reason": "to efficiently load large datasets without manual intervention"
      },
      {
        "role": "genomics researcher",
        "goal": "load only genes in my Ensembl filter list",
        "reason": "to reduce storage and improve query performance"
      },
      {
        "role": "data operations engineer",
        "goal": "resume processing after transient DB/network failures",
        "reason": "to avoid losing progress on long-running ETL jobs"
      },
      {
        "role": "system administrator",
        "goal": "configure ETL parameters without code changes",
        "reason": "to adapt the system to different environments"
      },
      {
        "role": "system administrator",
        "goal": "monitor ETL performance and data quality",
        "reason": "to identify bottlenecks and ensure reliable processing"
      },
      {
        "role": "database developer",
        "goal": "ensure proper foreign key relationships and surrogate keys",
        "reason": "to support accurate analytical joins"
      }
    ],
    "flows": [
      {
        "name": "Successful Multi-Study Processing",
        "steps": [
          "Startup: parse config.yaml into validated config models (Pydantic v2), initialize structlog JSON logger, create artifacts.db (SQLite) and run_id. ([pydantic.dev](https://pydantic.dev/articles/pydantic-v2-12-release?utm_source=openai))",
          "Connectivity: test SQL Server connection (if production mode); test local SQLite for artifacts.",
          "Load gene filter TSV into an in-memory hash set (or Polars Series) for O(1) lookups. ([pypi.org](https://pypi.org/project/polars/?utm_source=openai))",
          "Discover studies by scanning input_directory for subfolders with required TSVs; enqueue studies.",
          "Process N studies concurrently via ThreadPoolExecutor (N = max_concurrent_studies).",
          "For each study: read metadata TSV with Polars (lazy/streaming); normalize columns via mapping; coalesce to 'UNKNOWN' for missing values; upsert dims (study, platform, illness, gene, sample) in batches with unique constraints.",
          "Stream expression TSV by row: filter to target genes; pivot/stack to (sample, gene, value) tuples; insert fact_expression in batches.",
          "Commit each batch as its own transaction; record batch checkpoints in artifacts.db and in a DB checkpoint table.",
          "On completion, write per-study metrics and data-quality summary to artifacts.db and export to /logs as CSV/JSON.",
          "Exit with success code; print a run summary with counts and throughput."
        ]
      },
      {
        "name": "Recovery from Connection Failure",
        "steps": [
          "Detect DB error during batch insert; log error with current study, table, and batch index.",
          "Apply exponential backoff retry policy for reconnect attempts (tenacity). ([pypi.org](https://pypi.org/project/tenacity/?utm_source=openai))",
          "On successful reconnect, query last successful batch per study/table from checkpoints.",
          "Resume processing from the next batch boundary; ensure inserts are idempotent via unique constraints and ON CONFLICT/merge patterns.",
          "Verify no duplicates; continue to completion and record recovery metrics."
        ]
      },
      {
        "name": "Data Quality Handling",
        "steps": [
          "During metadata load, compute per-field completeness metrics (e.g., age, sex).",
          "Substitute missing/empty values with 'UNKNOWN' according to rules.",
          "During expression filtering, count total genes vs matched genes (skip logs for non-matches).",
          "Emit data-quality summary to artifacts.db and export CSV."
        ]
      },
      {
        "name": "Operational Snapshot and Rollback",
        "steps": [
          "At run start, generate run_id and persist snapshot metadata (config hash, file hashes).",
          "Tag all inserted rows (dims and fact) with run_id columns and capture per-batch row ranges in artifacts.db.",
          "If rollback invoked: compute affected batches; delete rows by run_id from fact first, then dims where no FK references remain.",
          "Record rollback event in artifacts.db; leave source TSVs intact (not data versioning)."
        ]
      }
    ],
    "risks": [
      "Very large TSVs may exceed available memory if not strictly streamed; ensure chunked/streaming reads.",
      "ODBC driver or network instability can cause intermittent failures; must tune retry/backoff and connection pooling.",
      "Deadlocks or lock escalation in SQL Server under concurrent writers; mitigate with batch sizing, proper indexes, and retry on 1205 errors. ([github.com](https://github.com/sqlalchemy/sqlalchemy/discussions/9436?utm_source=openai))",
      "Heterogeneous metadata columns may lead to mis-mapping; require dynamic field mapping and robust validation.",
      "Encoding inconsistencies (e.g., Latin-1 vs UTF-8) can break parsing; enforce UTF-8 and detect/convert when needed.",
      "Incorrect or outdated gene filter lists could exclude necessary data; logs must clearly show match counts.",
      "Duplicate handling depends on correct unique constraints; misconfigured indexes risk silent duplication or load failures.",
      "Performance variability across environments (disk, network, SQL Server edition) may impact SLA; provide tunables and profiling.",
      "Rollback may be partial if downstream tools read during load; define maintenance windows and use run_id tagging to isolate loads."
    ],
    "open_questions": [
      "Confirm target SQL Server version/edition and required ODBC driver version (e.g., ODBC Driver 17/18+) for production connectivity.",
      "Should we support Windows Authentication (Trusted_Connection) only, or also SQL auth/managed identity? Provide examples for each.",
      "What is the canonical file encoding for all TSVs (UTF-8 with LF)? Should we auto-detect and transcode non-UTF-8 sources?",
      "Are there upper bounds on study folder depth or naming deviations from the provided patterns?",
      "What are the SLAs for throughput (records/sec) and total wall-clock time per typical batch run?",
      "What retention policy should apply to artifacts.db and exported logs/reports (e.g., 30–90 days)?",
      "Is operational rollback limited to the most recent run per study or any historical run within retention?",
      "Should we pre-create the analytical schema and indexes via migrations, or create on first run if absent?",
      "Are there privacy/compliance constraints for logs (e.g., masking GSM accessions or PII fields)?",
      "Do we need to support non-human organisms or multiple organisms in the same run (filter list may contain organism column)?"
    ]
  },
  "raw": {
    "prompt": "Please create a requirements file from the spec",
    "input_docs_text": "# Genomic Expression Data ETL - Requirements Document\n\n**Document Version**: 1.0  \n**Date**: October 14, 2025  \n**Requirements Analyst**: Technical Specifications  \n**Status**: Final Requirements  \n\n## Overview\n\nThis document defines the functional and technical requirements for a genomic expression data Extract, Transform, and Load (ETL) system. The ETL will process multiple genomic studies from TSV files containing expression data and metadata, filter the data based on predefined Ensembl gene identifiers, and populate a normalized database schema for analytical querying.\n\nThe system is designed to handle the complexities of genomic research data, including inconsistent metadata schemas, large datasets, and the need for reliable batch processing with error recovery capabilities.\n\n## Goals\n\n### Primary Goals\n1. **Automated Data Processing**: Create a robust ETL pipeline that processes multiple genomic studies concurrently without manual intervention\n2. **Data Quality Management**: Transform messy, inconsistent genomic metadata into clean, standardized dimensional tables  \n3. **Performance Optimization**: Filter large expression datasets during ingestion to process only relevant genes, improving memory efficiency and processing speed\n4. **System Reliability**: Provide resilient processing with connection failure recovery and idempotent operations\n5. **Analytical Database Population**: Populate a star schema database optimized for genomic expression analysis\n\n### Secondary Goals\n- Configurable processing parameters for different deployment environments\n- Comprehensive monitoring and logging for operational visibility\n- Scalable architecture supporting varying study sizes and concurrent processing\n\n## Assumptions\n\n### Data Assumptions\n- **File Format**: All input files are in TSV (tab-separated values) format\n- **Study Organization**: Each study resides in its own directory with standardized file naming conventions\n- **Metadata Variability**: Different studies may have varying metadata schemas, but all contain core required fields\n- **Gene Identification**: All expression data uses Ensembl gene identifiers (format: ENSG00000XXXXXXX)\n- **Data Volume**: Individual studies may contain 10,000-50,000 samples with expression data for 20,000+ genes\n\n### Infrastructure Assumptions  \n- SQL Server database with sufficient capacity for dimensional tables and fact data\n- File system access to organized study directories\n- Network connectivity allowing resume capability after temporary disconnections\n- Sufficient memory for batch processing (minimum 8GB recommended)\n\n### Processing Assumptions\n- Studies can be processed independently and concurrently\n- Gene filtering reduces dataset size by approximately 99% (from ~20,000 genes to ~120 target genes)\n- Database supports batch inserts and transaction management\n- Configuration files are maintained and accessible to the ETL process\n\n## Requirements\n\n### MUST HAVE Requirements\n\n#### Core ETL Functionality\n\n**REQ-001: Multi-Study Processing**\n- **Description**: The system must process multiple genomic studies concurrently from an organized folder structure\n- **Details**: \n  - Each study folder contains: expression data TSV file, metadata TSV file\n  - System scans root directory for study folders automatically\n  - Concurrent processing of multiple studies with configurable thread limits\n- **Acceptance Criteria**: Successfully processes at least 5 studies concurrently without data corruption\n\n**REQ-002: Gene Filtering During Load**  \n- **Description**: The system must filter expression data by Ensembl gene IDs during data ingestion to optimize performance\n- **Details**:\n  - Load gene filter list from external configuration file\n  - Filter format: `ensembl_id` column containing ENSG identifiers\n  - Skip non-matching genes without individual logging\n  - Apply filtering before loading into memory structures\n- **Input Example**: Filter file contains genes like `ENSG00000115977`, `ENSG00000112304`, `ENSG00000093072`\n- **Acceptance Criteria**: Only genes present in filter list are processed and loaded to database\n\n**REQ-003: Database Schema Population**\n- **Description**: The system must populate all five dimensional tables with proper foreign key relationships\n- **Target Schema**:\n  ```sql\n  -- Fact table\n  fact_expression: sample_key, gene_key, expression_value, study_key\n  \n  -- Dimension tables  \n  dim_gene: gene_key (identity), ensembl_id\n  dim_sample: sample_key (identity), gsm_accession, study_key, platform_key, illness_key, age, sex\n  dim_study: study_key (identity), gse_accession  \n  dim_illness: illness_key (identity), illness_label\n  dim_platform: platform_key (identity), platform_accession\n  ```\n- **Acceptance Criteria**: All tables populated with correct data types and valid foreign key relationships\n\n**REQ-004: Data Transformation Logic**\n- **Description**: The system must apply specific transformation rules for metadata extraction and standardization\n- **Source-to-Target Mapping**:\n  ```\n  # Sample Metadata Mapping\n  gsm_accession ← refinebio_accession_code\n  age ← refinebio_age OR characteristics_ch1_Age  \n  sex ← refinebio_sex OR characteristics_ch1_Sex OR characteristics_ch1_Gender\n  illness_label ← characteristics_ch1_Illness\n  platform_accession ← refinebio_platform\n  gse_accession ← experiment_accession (study level)\n  \n  # Expression Data Mapping  \n  ensembl_id ← Gene column (first column in expression file)\n  expression_value ← Sample columns (GSM228562, GSM228563, etc.)\n  ```\n- **Missing Value Handling**: Replace empty/null values with \"UNKNOWN\" string\n- **Acceptance Criteria**: Transformed data matches expected mappings with proper \"UNKNOWN\" substitution\n\n**REQ-005: Duplicate Handling**\n- **Description**: The system must skip duplicate records on re-processing while maintaining data integrity\n- **Duplicate Detection Logic**:\n  - **Studies**: Skip if `gse_accession` already exists in dim_study\n  - **Samples**: Skip if combination of `gsm_accession` + `study_key` exists in dim_sample  \n  - **Genes**: Skip if `ensembl_id` already exists in dim_gene\n  - **Expression Facts**: Skip if combination of `sample_key` + `gene_key` + `study_key` exists\n- **Acceptance Criteria**: Re-running ETL on same data produces no duplicate records\n\n**REQ-006: Batch Processing & Transaction Management**\n- **Description**: The system must process data in configurable batches with proper transaction handling\n- **Batch Configuration**:\n  - Default batch size: 1000 records  \n  - Configurable via configuration file\n  - Separate batch processing for each table type\n- **Transaction Scope**: Each batch commits as separate transaction for memory management\n- **Acceptance Criteria**: System processes 10,000+ records in 1000-record batches without memory issues\n\n**REQ-007: Resume Capability**  \n- **Description**: The system must resume processing after database connection failures without restarting entire studies\n- **Resume Logic**:\n  - Track processing state per study in database or state file\n  - Detect incomplete studies on startup  \n  - Resume from last successful batch commit\n  - Handle both connection timeouts and network failures\n- **Acceptance Criteria**: System resumes processing after simulated connection failure without data loss or duplication\n\n**REQ-008: Configuration Management**\n- **Description**: The system must use external configuration files for all deployment-specific parameters\n- **Configuration Parameters**:\n  ```yaml\n  # Example config.yaml\n  database:\n    connection_string: \"Server=localhost;Database=db_genes;Trusted_Connection=true;\"\n    batch_size: 1000\n    connection_timeout: 30\n    \n  processing:\n    input_directory: \"/data/studies\"\n    gene_filter_file: \"/config/filter_genes.tsv\"\n    max_concurrent_studies: 3\n    \n  logging:\n    log_level: \"INFO\"\n    log_directory: \"/logs\"\n    log_processing_time: true\n    log_record_counts: true\n    log_data_quality: true\n  ```\n- **Acceptance Criteria**: All parameters configurable without code changes\n\n#### Monitoring & Logging\n\n**REQ-009: Process Monitoring**  \n- **Description**: The system must provide comprehensive logging for operations monitoring\n- **Required Logging**:\n  - Processing start/end time per study\n  - Record counts (loaded, filtered, errors) per table per study  \n  - Data quality issues (missing required fields, format problems)\n  - Performance metrics (records/second, memory usage)\n  - **Excluded**: Individual skipped gene records (performance optimization)\n- **Acceptance Criteria**: Log files contain sufficient information for troubleshooting and performance monitoring\n\n### SHOULD HAVE Requirements\n\n#### Enhanced Error Handling\n\n**REQ-010: Validation Reporting**\n- **Description**: The system should generate summary reports of data quality issues per study\n- **Report Contents**:\n  - Percentage of samples with complete metadata\n  - Count of genes filtered vs. total genes in source\n  - List of studies with processing warnings\n  - Performance statistics (processing time, throughput)\n- **Acceptance Criteria**: Summary report generated after each ETL run\n\n**REQ-011: File Format Validation**  \n- **Description**: The system should validate TSV file structure before processing\n- **Validation Checks**:\n  - Verify required columns exist in metadata files\n  - Check expression file has gene column plus sample columns\n  - Validate file encoding and delimiter consistency  \n  - Confirm file size within expected ranges\n- **Acceptance Criteria**: Processing stops with clear error message for invalid file formats\n\n**REQ-012: Performance Optimization**\n- **Description**: The system should support parallel processing optimizations for large datasets\n- **Optimization Features**:\n  - Parallel study processing with configurable thread pools\n  - Bulk insert operations for database loading\n  - Memory-efficient streaming for large TSV files\n  - Connection pooling for database operations\n- **Acceptance Criteria**: 50% improvement in processing time for multiple concurrent studies\n\n#### Configuration Enhancements\n\n**REQ-013: Dynamic Column Mapping**\n- **Description**: The system should support configurable metadata field mappings for different study types  \n- **Configuration Example**:\n  ```yaml\n  field_mappings:\n    age_fields: [\"refinebio_age\", \"characteristics_ch1_Age\", \"characteristics_ch1_age\", \"MetaSRA_age\"]\n    sex_fields: [\"refinebio_sex\", \"characteristics_ch1_Sex\", \"characteristics_ch1_Gender\"]\n    illness_fields: [\"characteristics_ch1_Illness\", \"refinebio_disease\"]\n    platform_fields: [\"refinebio_platform\", \"platform_id\"]\n  ```\n- **Acceptance Criteria**: System successfully processes studies with varying metadata column names\n\n### WON'T HAVE Requirements\n\n#### Explicitly Out of Scope\n\n**REQ-014: Real-time Processing**  \n- **Rationale**: Genomic research workflows operate on batch schedules; real-time streaming adds unnecessary complexity\n- **Alternative**: Scheduled batch processing meets research requirements\n\n**REQ-015: Advanced Data Validation**\n- **Rationale**: Research requirement to import expression data \"as-is\" without range validation  \n- **Alternative**: Downstream analysis tools handle data quality validation\n\n**REQ-016: Web Interface**\n- **Rationale**: Technical users prefer command-line automation and scripting capabilities\n- **Alternative**: Command-line interface with comprehensive logging\n\n**REQ-017: Data Versioning**  \n- **Rationale**: Source TSV files serve as system of record; database versioning adds storage overhead\n- **Alternative**: Maintain source file archives for historical analysis\n\n**REQ-018: Custom Export Formats**\n- **Rationale**: Database serves as single analytical data store; export features not required\n- **Alternative**: Direct database query access for downstream tools\n\n## User Stories\n\n### Data Analyst User Stories\n\n**US-001: Process Multiple Studies**\n> **As a** genomics data analyst  \n> **I want to** process multiple expression studies simultaneously  \n> **So that** I can efficiently load large datasets without manual intervention  \n>\n> **Acceptance Criteria:**\n> - I can configure the system to process 5 studies concurrently\n> - Each study processes independently without interfering with others  \n> - I receive a summary report showing processing results for all studies\n> - Processing completes in under 2 hours for typical study sizes\n\n**US-002: Filter Relevant Genes**  \n> **As a** genomics researcher  \n> **I want to** load only genes relevant to my analysis  \n> **So that** I can reduce storage requirements and improve query performance\n>\n> **Acceptance Criteria:**\n> - I can specify a gene filter file containing my target Ensembl IDs\n> - The system loads only expression data for genes in my filter list\n> - I can see in the logs how many genes were filtered vs. total genes available\n> - Database queries run faster due to reduced data volume\n\n**US-003: Handle Connection Failures**\n> **As a** data operations engineer  \n> **I want to** resume processing after database connection failures  \n> **So that** long-running ETL jobs don't fail completely due to temporary network issues\n>\n> **Acceptance Criteria:**  \n> - If database connection is lost during processing, the system detects the failure\n> - The system waits and retries connection with exponential backoff\n> - Processing resumes from the last successful batch commit\n> - No duplicate data is created when resuming\n\n### System Administrator User Stories\n\n**US-004: Configure Processing Parameters**\n> **As a** system administrator  \n> **I want to** configure ETL parameters without modifying code  \n> **So that** I can adapt the system to different environments and requirements\n>\n> **Acceptance Criteria:**\n> - All database connections, file paths, and processing options are in config files\n> - I can change batch sizes, concurrent processing limits, and timeout values  \n> - Configuration changes take effect without recompilation\n> - Invalid configuration values produce clear error messages\n\n**US-005: Monitor Processing Performance**  \n> **As a** system administrator  \n> **I want to** monitor ETL performance and data quality  \n> **So that** I can identify bottlenecks and ensure reliable data processing\n>\n> **Acceptance Criteria:**\n> - Log files show processing time, record counts, and error rates for each study\n> - I can track memory usage and database performance during large runs\n> - Data quality issues are logged with sufficient detail for investigation  \n> - Performance trends are visible across multiple ETL runs\n\n### Database Developer User Stories\n\n**US-006: Populate Dimensional Schema**\n> **As a** database developer  \n> **I want to** ensure proper foreign key relationships in the loaded data  \n> **So that** analytical queries join correctly across dimensional tables\n>\n> **Acceptance Criteria:**\n> - All foreign keys in fact_expression reference valid dimension records\n> - Dimension tables contain no orphaned records  \n> - Surrogate keys are properly generated and unique across studies\n> - Database constraints validate referential integrity\n\n## Example Processing Flows\n\n### Flow 1: Successful Multi-Study Processing\n\n```\n1. System Startup\n   ├── Load configuration from config.yaml\n   ├── Validate database connectivity  \n   ├── Load gene filter list (120 target genes)\n   └── Scan /data/studies directory\n\n2. Discovery Phase  \n   ├── Found studies: GSE9006/, GSE10201/, GSE15061/\n   ├── Validate each study has required TSV files\n   └── Queue studies for processing\n\n3. Concurrent Processing (max 3 studies)\n   ├── Study GSE9006: Start processing\n   │   ├── Load metadata_GSE9006.tsv (163 samples)\n   │   ├── Extract sample dimensions → dim_sample, dim_illness, dim_platform\n   │   ├── Load expression_GSE9006.tsv (22,283 genes) \n   │   ├── Filter to target genes (120 genes matched)\n   │   └── Load expression facts (163 samples × 120 genes = 19,560 records)\n   │\n   ├── Study GSE10201: Start processing (parallel)\n   └── Study GSE15061: Start processing (parallel)\n\n4. Batch Commits (per study)\n   ├── Commit dim_sample records in batches of 1000\n   ├── Commit fact_expression records in batches of 1000  \n   └── Update processing state after each successful batch\n\n5. Completion\n   ├── Generate summary report\n   ├── Log final statistics\n   └── Exit with success code\n```\n\n### Flow 2: Recovery from Connection Failure\n\n```\n1. Processing in Progress\n   ├── Study GSE9006: 50% complete (9,780 expression records loaded)\n   ├── Study GSE10201: 25% complete  \n   └── Database connection lost\n\n2. Error Detection & Recovery\n   ├── Detect connection failure\n   ├── Log error with current processing state\n   ├── Wait 30 seconds, attempt reconnection\n   └── Successful reconnection established\n\n3. Resume Processing  \n   ├── Query database for last successful batch per study\n   ├── Study GSE9006: Resume from record 9,801  \n   ├── Study GSE10201: Resume from record 2,501\n   └── Continue processing remaining records\n\n4. Validation\n   ├── Verify no duplicate records created\n   ├── Confirm all expected records loaded\n   └── Complete processing normally\n```\n\n### Flow 3: Data Quality Handling\n\n```\n1. Metadata Processing\n   ├── Load metadata_GSE9006.tsv\n   ├── Sample GSM228562: characteristics_ch1_Age = \"16\"\n   ├── Sample GSM228563: characteristics_ch1_Age = \"\" (empty)\n   │   └── Set age = \"UNKNOWN\" \n   ├── Sample GSM228564: characteristics_ch1_Illness = \"Healthy\"\n   └── Sample GSM228565: characteristics_ch1_Sex missing\n       └── Set sex = \"UNKNOWN\"\n\n2. Gene Filtering\n   ├── Expression file contains 22,283 genes\n   ├── Gene ENSG00000115977 (AAK1): Match in filter → Include\n   ├── Gene ENSG00000000003: Not in filter → Skip (no log)\n   ├── Gene ENSG00000112304 (ACOT13): Match in filter → Include\n   └── Final result: 120 genes processed, 22,163 genes skipped\n\n3. Quality Reporting\n   ├── Log: \"Processed 163 samples, 120 genes, 19,560 expression records\"\n   ├── Log: \"Data quality: 89% samples with complete age, 92% with complete sex\"\n   └── Log: \"Processing time: 4.2 minutes, 78 records/second\"\n```\n\n## Directory Structure Examples\n\n### Input Directory Structure\n```\n/data/studies/\n├── GSE9006/\n│   ├── metadata_GSE9006.tsv          # Sample metadata  \n│   └── expression_GSE9006.tsv        # Expression data\n├── GSE10201/\n│   ├── metadata_GSE10201.tsv\n│   └── expression_GSE10201.tsv  \n├── GSE15061/\n│   ├── metadata_GSE15061.tsv\n│   └── expression_GSE15061.tsv\n└── GSE20142/\n    ├── metadata_GSE20142.tsv\n    └── expression_GSE20142.tsv\n```\n\n### Configuration File Structure  \n```\n/config/\n├── config.yaml                      # Main configuration\n├── filter_genes.tsv                 # Target gene list\n└── field_mappings.yaml              # Metadata column mappings (optional)\n\n/logs/\n├── etl_20251014_102300.log          # Main processing log\n├── data_quality_20251014.csv        # Quality summary report  \n└── performance_20251014.json        # Performance metrics\n```\n\n### Sample File Formats\n\n#### Filter Genes File (filter_genes.tsv)\n```\ngene_symbol\tensembl_id\trefinebio_organism\tgene_name\nAAK1\tENSG00000115977\tHomo sapiens\tAP2 associated kinase 1\nACOT13\tENSG00000112304\tHomo sapiens\tacyl-CoA thioesterase 13\nADA2\tENSG00000093072\tHomo sapiens\tadenosine deaminase 2\n```\n\n#### Expression Data File (expression_GSE9006.tsv)  \n```\nGene\tGSM228562\tGSM228563\tGSM228564\tGSM228565\nENSG00000115977\t0.717\t0.504\t0.581\t0.886\nENSG00000112304\t0.562\t0.596\t0.628\t0.739  \nENSG00000093072\t0.429\t0.466\t0.533\t0.337\n```\n\n#### Metadata File (metadata_GSE9006.tsv)\n```\nrefinebio_accession_code\texperiment_accession\trefinebio_age\trefinebio_sex\trefinebio_platform\tcharacteristics_ch1_Age\tcharacteristics_ch1_Sex\tcharacteristics_ch1_Illness\nGSM228562\tGSE9006\t16\tfemale\tGPL96\t16 yrs\tF\tHealthy\nGSM228563\tGSE9006\t\tfemale\tGPL96\t16 yrs\tF\tHealthy  \nGSM228564\tGSE9006\t25\tmale\tGPL96\t25 yrs\tM\tUNKNOWN\n```\n\n---\n\n**Document Approval:**  \nRequirements Analyst: [Signature]  \nDate: October 14, 2025  \n\n**Change Control:**  \nVersion 1.0 - Initial requirements document  \nNext Review Date: November 14, 2025"
  }
}